### Exercise 6.1

This question relates to the `College` dataset from the `ISLR` package.

(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform appropriate model selection of your choice (from day6) on the training set in order to identify a satisfactory model that uses just a subset of the predictors.

```{r, }
library(ISLR)
library(glmnet)
library(leaps)  
```

```{r}
set.seed(11)

# check for NA's in the data
sum(is.na(College))

train.size <-  nrow(College) / 2
# there are 777 rows, we are specifying the training size to be 1/2 that. However, it's best to use nrow() to ensure you don't miscalculate or have the wrong number. 

train.size  #388.5 

train <-  sample(1:nrow(College), train.size)
# basically this is sample(1:777, 388.5) which provides 388 integers between the 1:777 randomly sampled but reproducible because of set.seed()

test <-  -train
# same values but negative

College.train <-  College[train, ]
# this takes the number indices from train and subsets the College data into the training set

College.test <-  College[test, ]
# negative indices are treated like NOT these. You can also have skipped the test <- -train and just do the code like this:
College.test <- College[-train,]

# next we will make model matrices for test and train w/o intercept (Outstate in this case), basically just the right side of the linear regression formula
train.mat <-  model.matrix(Outstate ~ . , data = College.train)
test.mat <-  model.matrix(Outstate ~ . , data = College.test)


#exponentiate the sequence, which is from 4 to -2, length is set to 100, length lamda 
# lambda is when you are running the lasso model, lasso is the implemented punishment for overfitting. You define this with the grid, so in this case there are 100 dfiferent values of punishment.

grid <-  10^ seq(4, -2, length = 100)
# seq() makes a sequence of 100 numbers long (as specified) that starts at 4 and ends at -2
# 10^ each number in the sequence, so really goes from 10000 to 0.01
# Note: this does not mean that it's the same as doing seq(10000, 0.01, length = 100)

#to see outside of scientific notation
options(scipen=6)
grid
```


```{r}
# glmnet() is an R package which can be used to fit Regression models, lasso model and others. 

#"Note that cv.glmnet does NOT search for values for alpha. A specific value should be supplied, else alpha=1 is assumed by default."

# k-fold cross-validation for glmnet with cv.glmnet()
# Can also rely on a default lambda chosen by glmnet sequence (see documentation)
mod.lasso <-  cv.glmnet(train.mat, College.train[, "Outstate"], alpha = 1, 
                        lambda = grid, thresh = 1e-12)
# 1e-12 is scientific notation for 1,000,000,000,000
# default in the function for thresh = 1e-7 or 10,000,000
# Alpha argument determines what type of model is fit. When alpha=0, Ridge Model is fit and if alpha=1, a lasso model is fit. 
# Remember, train.mat is right side of regression model, College.train is the left hand side of the regression model in the matrix format. (Y)

lambda.best <-  mod.lasso$lambda.min
#This is the optimal lamda selected for cross-validation
lambda.best

# you can also take a broader look at the min and 1se Lambda by just running:
mod.lasso


# Lasso shrinks the coefficient estimates towards zero and it has the effect of setting variables exactly equal to zero when lambda is large enough while ridge does not. Hence, much like the best subset selection method, lasso performs variable selection. The tuning parameter lambda is chosen by cross validation.

# We can use glmnet to take a look at how coefficients are shrunk at different values of lambda.
lasso <- glmnet(train.mat, College.train[, "Outstate"], alpha = 1, lambda = grid)

lasso.coef <-  predict(lasso, type= "coefficients", s = lambda.best)
# using predict() to run the model and get the coeffecients using the best lamda from the cross-validation model. s= Value(s) of the penalty parameter lambda at which predictions are required. Default is the entire sequence used to create the model.
lasso.coef

# you can use plot() easily thanks to glmnet()
plot(lasso, xvar="lambda", label=T)

```

**LASSO hasn't dropped any variables from our model, so it was a very successful attempt at model selection. Other approaches have been covered in the chapter (although not in the lecture). Best subset selection is a brute force approach but may be useful if we are determined in reducing the size of our model.**


```{r}
set.seed(1)

train <-  sample(1:nrow(College), nrow(College)/2)
test <-  -train
College.train <-  College[train, ]
College.test <-  College[test, ]

# forward stepwise regression, start with one variable and then add
reg.fit <-  regsubsets(Outstate~., data=College.train, nvmax=17, method="forward")

#nvmax is maximum size of subsets to examine
reg.summary <-  summary(reg.fit)
```

```{r}
#Various statistics can be used to judge the quality of a model. These include Mallowâ€™s Cp (first plot), Akaike information criterion (AIC), and Bayesian information criterion (BIC) (second plot)
par(mfrow=c(1, 3))
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
min.cp <-  min(reg.summary$cp)
std.cp <-  sd(reg.summary$cp)
abline(h=min.cp+0.2*std.cp, col="red", lty=2)
abline(h=min.cp-0.2*std.cp, col="red", lty=2)

plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
min.bic <-  min(reg.summary$bic)
std.bic <-  sd(reg.summary$bic)
abline(h=min.bic+0.2*std.bic, col="red", lty=2)
abline(h=min.bic-0.2*std.bic, col="red", lty=2)

plot(reg.summary$adjr2,xlab="Number of Variables",
     ylab="Adjusted R2",type='l', ylim=c(0.4, 0.84))
max.adjr2 <-  max(reg.summary$adjr2)
std.adjr2 <-  sd(reg.summary$adjr2)
abline(h=max.adjr2+0.2*std.adjr2, col="red", lty=2)
abline(h=max.adjr2-0.2*std.adjr2, col="red", lty=2)
```

**BIC scores show 6 as the optimal size. Cp, BIC and adjr2 show that size 6 is the minimum size for the subset for which the scores are withing 0.2 standard deviations of optimum. We pick 6 as the best subset size and find best 6 variables using entire data.**

```{r}
reg.fit <-  regsubsets(Outstate ~ . , data=College, method="forward")
coefi <-  coef(reg.fit, id=6)
#id=6 is picking 6 as the subset size to find the best 6 variables
names(coefi)
```


(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.

```{r}
library(gam)
# non parametric regression - 
gam.fit <-  gam(Outstate ~ Private + ns(Room.Board, df=2) + 
                  ns(PhD, df=2) + ns(perc.alumni, df=2) + 
                  ns(Expend, df=5) + ns(Grad.Rate, df=2),
                data=College.train)
# ns is number of splines

par(mfrow=c(2, 3))
plot(gam.fit, se=TRUE, col="blue")
```

** We discussed this type of graphs in the lecture. The fitted natural splines with +/- 2*SE confidence interval. Ticks at the bottom show density of the data (aka `rug plot').**

(c) Evaluate the model obtained on the test set, and explain the results obtained.

```{r}
gam.pred <-  predict(gam.fit, College.test)
gam.err <-  mean((College.test$Outstate - gam.pred)^2)
gam.err
gam.tss <-  mean((College.test$Outstate - mean(College.test$Outstate))^2)
test.rss <-  1 - gam.err / gam.tss
test.rss
```

**We obtain a test RSS of 0.76 using GAM with 6 predictors. This is a slight improvement over a test RSS of 0.74 obtained using OLS.** 

(d) For which variables, if any, is there evidence of a non-linear relationship with the response?

```{r}
summary(gam.fit)
```

**Non-parametric Anova test shows a strong evidence of non-linear relationship between response and variables.** 


### Exercise 6.2 

Apply bagging and random forests to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance?

**In this exercise we examine the `Weekly` stock market data from the ISLR package.**

```{r}
set.seed(1)
summary(Weekly)
train <-  sample(nrow(Weekly), 2/3 * nrow(Weekly))
test <-  -train
```

**Logistic regression**

```{r}
glm.fit <-  glm(Direction ~ . -Year-Today, 
                data=Weekly[train,], 
                family="binomial")

glm.probs <-  predict(glm.fit, newdata=Weekly[test, ], 
                      type = "response")
glm.pred <-  rep("Down", length(glm.probs))
# repeating "Down" the length of glm.probs, to create confusion matrix

glm.pred[glm.probs > 0.5] <-  "Up"
table(glm.pred, Weekly$Direction[test])

# Rows are predicted, columns are true
# percentage of those predicted incorrectly
mean(glm.pred != Weekly$Direction[test])

```


**Bagging**

```{r}
library(randomForest)

Weekly <-  Weekly[,!(names(Weekly) %in% c("BinomialDirection"))]

bag.weekly <-  randomForest(Direction~.-Year-Today, 
                            data=Weekly, 
                            subset=train, 
                            mtry=6)
# mtry is the number of variable you are randomly selecting

yhat.bag <-  predict(bag.weekly, newdata=Weekly[test,])
table(yhat.bag, Weekly$Direction[test])
mean(yhat.bag != Weekly$Direction[test])
```

**Random forests**

```{r}
rf.weekly <-  randomForest(Direction ~ . -Year-Today, 
                           data=Weekly, 
                           subset=train, 
                           mtry=2)

yhat.bag <-  predict(rf.weekly, newdata=Weekly[test,])
table(yhat.bag, Weekly$Direction[test])
mean(yhat.bag != Weekly$Direction[test])

```

**Best performance summary: Bagging resulted in the lowest validation set test error rate.**
